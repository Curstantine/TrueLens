{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hdbscan-0.8.29-cp313-cp313-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 60/60 [00:43<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Completed.\n",
      "Grouped articles saved to D:\\sdgpTruelens\\TrueLens\\news_filtered_data\\news_source_data\\data\\grouped_articles.json.\n",
      "Stories created and factuality calculated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Find the repository root dynamically\n",
    "REPO_ROOT = Path(os.getcwd()).resolve()\n",
    "while REPO_ROOT.name != \"TrueLens\" and REPO_ROOT != REPO_ROOT.parent:\n",
    "    REPO_ROOT = REPO_ROOT.parent  # Move up until we reach the repo root\n",
    "\n",
    "# Define paths relative to the repository root\n",
    "ARTICLES_DIR = REPO_ROOT / \"news_filtered_data/news_source_data/data/articles\"\n",
    "OUTPUT_FILE = REPO_ROOT / \"news_filtered_data/news_source_data/data/grouped_articles.json\"\n",
    "\n",
    "def is_ascii(s):\n",
    "    \"\"\" Check if the string contains only ASCII characters. \"\"\"\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def load_articles(directory):\n",
    "    \"\"\" Load articles from JSON files and filter out empty ones. \"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    for file in Path(directory).rglob(\"*.json\"):  # Recursively find all JSON files\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            text = \" \".join(data.get(\"body_paragraphs\", [])).strip()  # Combine and clean text\n",
    "            \n",
    "            # Skip empty articles and non-ASCII titles\n",
    "            if text and is_ascii(data[\"title\"]):\n",
    "                articles.append({\n",
    "                    \"url\": data[\"url\"],\n",
    "                    \"title\": data[\"title\"],\n",
    "                    \"ut\": data[\"ut\"],\n",
    "                    \"body_paragraphs\": text,\n",
    "                    \"outlet\": data.get(\"outlet\", \"unknown\"),\n",
    "                    \"reporter\": data.get(\"reporter\", None)\n",
    "                })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def cluster_articles(articles, num_clusters=5):\n",
    "    \"\"\" Cluster articles using BERTopic. \"\"\"\n",
    "    body_paragraphs = [article[\"body_paragraphs\"] for article in articles]\n",
    "    \n",
    "    # Handle case where all documents are empty\n",
    "    if not any(body_paragraphs):\n",
    "        raise ValueError(\"No valid articles found for clustering.\")\n",
    "    \n",
    "    # Use a pre-trained SentenceTransformer model for embeddings\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(body_paragraphs, show_progress_bar=True)\n",
    "    \n",
    "    # Create and fit BERTopic model\n",
    "    topic_model = BERTopic(nr_topics=num_clusters)\n",
    "    topics, _ = topic_model.fit_transform(body_paragraphs, embeddings)\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        article[\"cluster_id\"] = int(topics[i])\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def save_grouped_articles(articles, output_file):\n",
    "    \"\"\" Save grouped articles to a JSON file and return grouped data. \"\"\"\n",
    "    grouped_articles = {}\n",
    "    for article in articles:\n",
    "        cluster_id = article[\"cluster_id\"]\n",
    "        if cluster_id not in grouped_articles:\n",
    "            grouped_articles[cluster_id] = []\n",
    "        grouped_articles[cluster_id].append(article)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(grouped_articles, f, indent=2)\n",
    "    \n",
    "    return grouped_articles  # Return the dictionary\n",
    "\n",
    "def create_reporter_and_outlet(article):\n",
    "    \"\"\" Create reporter and outlet information. \"\"\"\n",
    "    reporter = article[\"reporter\"]\n",
    "    outlet = article[\"outlet\"]\n",
    "    \n",
    "    if not reporter:\n",
    "        reporter = f\"system-{outlet}\"\n",
    "        is_system = True\n",
    "    else:\n",
    "        is_system = False\n",
    "    \n",
    "    return {\n",
    "        \"reporter\": reporter,\n",
    "        \"is_system\": is_system,\n",
    "        \"outlet\": outlet\n",
    "    }\n",
    "\n",
    "def process_articles(articles):\n",
    "    \"\"\" Process articles to add reporter and outlet information. \"\"\"\n",
    "    for article in articles:\n",
    "        reporter_info = create_reporter_and_outlet(article)\n",
    "        article.update(reporter_info)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def create_story_and_calculate_factuality(grouped_articles):\n",
    "    \"\"\" Create stories and calculate factuality for clustered articles. \"\"\"\n",
    "    # Mock trpcClient for demonstration purposes\n",
    "    class MockTRPCClient:\n",
    "        class Story:\n",
    "            @staticmethod\n",
    "            def create(data):\n",
    "                return {\"id\": 1, \"title\": data[\"title\"], \"articles\": data[\"articles\"]}\n",
    "        \n",
    "        class Article:\n",
    "            @staticmethod\n",
    "            def update(data):\n",
    "                pass\n",
    "    \n",
    "    trpcClient = MockTRPCClient()\n",
    "    \n",
    "    for cluster_id, articles in grouped_articles.items():\n",
    "        # Create a new story\n",
    "        story = trpcClient.Story.create({\n",
    "            \"title\": f\"Story for cluster {cluster_id}\",\n",
    "            \"articles\": [article[\"url\"] for article in articles]\n",
    "        })\n",
    "        \n",
    "        # Calculate factuality (assuming a function or service for this)\n",
    "        factuality = calculate_factuality(articles)\n",
    "        \n",
    "        # Update articles with story ID and factuality\n",
    "        for article in articles:\n",
    "            trpcClient.Article.update({\n",
    "                \"url\": article[\"url\"],\n",
    "                \"storyId\": story[\"id\"],\n",
    "                \"factuality\": factuality\n",
    "            })\n",
    "\n",
    "def calculate_factuality(articles):\n",
    "    \"\"\" Dummy function to calculate factuality. Replace with actual implementation. \"\"\"\n",
    "    return 75  # Placeholder value\n",
    "\n",
    "# Run the process\n",
    "articles = load_articles(ARTICLES_DIR)\n",
    "\n",
    "if articles:\n",
    "    clustered_articles = cluster_articles(articles, num_clusters=min(5, len(articles)))  # Ensure we don't request more clusters than articles\n",
    "    print(\"Clustering Completed.\")\n",
    "    \n",
    "    processed_articles = process_articles(clustered_articles)\n",
    "    grouped_articles = save_grouped_articles(processed_articles, OUTPUT_FILE)  # Capture returned dictionary\n",
    "    print(f\"Grouped articles saved to {OUTPUT_FILE}.\")\n",
    "\n",
    "    # Now, pass the correctly formatted dictionary\n",
    "    create_story_and_calculate_factuality(grouped_articles)\n",
    "    print(\"Stories created and factuality calculated.\")\n",
    "else:\n",
    "    print(\"No valid articles found. Process terminated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
